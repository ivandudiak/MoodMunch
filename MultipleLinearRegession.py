# -*- coding: utf-8 -*-
"""Copy of Копія записника "Untitled0.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RlFap8Uds9cZsBaSbyH76LQvp9IfhirQ
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
from sklearn.datasets import load_digits
from matplotlib import pyplot as plt
import datetime

def TrainTestSplit(X,y,p,seed=1):
  '''Splits feature matrix X and target array y into train and test sets
  p is the fraction going to train'''
  np.random.seed(seed) #controls randomness
  size=len(y) 
  train_size=int(p*size)
  train_mask=np.zeros(size,dtype=bool)
  train_indices=np.random.choice(size, train_size, replace=False)
  train_mask[train_indices]=True 
  test_mask=~train_mask
  X_train=X[train_mask]
  X_test=X[test_mask]
  y_train=y[train_mask]
  y_test=y[test_mask]
  return X_train,X_test,y_train,y_test

def PolyFeatures(x,d):
  X=np.zeros((len(x),d+1))
  for i in range(d+1):
    X[:,i]=x**i
  return X

def AddOnes(X):
  return np.concatenate((X,np.ones((len(X),1))),axis=1)

class Scaler:
  def __init__(self,z):
    self.min=np.min(z,axis=0)
    self.max=np.max(z,axis=0)

  def scale(self,x):
    
    return (x-self.min)/(self.max-self.min+0.000001)

  def unscale(self,x):
    return x*(self.max-self.min+0.000001)+self.min


def LinearSGD(X,y,epochs,batch_size,lr,alpha=0,beta=0):
  '''Stochastic Gradient Descent With L1 and L2 regularization'''
  #alpha=amount of L1 (Lasso) regularization
  #beta=amount of L2 (Ridge) regularization
  X=np.array(X) #Just in case X is a DataFrame
  y=np.array(y) #Just in case y is a Series
  n=len(X)
  coeff=np.ones(X.shape[1]) #Initialize all coeff to be 1 (something to play with?)
  indices=np.arange(len(X))
  for i in range(epochs):
    np.random.seed(i)
    np.random.shuffle(indices)
    X_shuffle=X[indices] 
    y_shuffle=y[indices] 
    num_batches=n//batch_size
    for j in range(num_batches):
      X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]
      y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]
      resid=X_batch@coeff-y_batch
      gradient=lr*((X_batch.T)@resid)/len(X_batch)+alpha*(2*(coeff>0)-1)+beta*coeff
      coeff=coeff-gradient #Gradient Descent step.
    if n%batch_size!=0: #Check if there is a smaller leftover batch
      X_batch=X_shuffle[num_batches*batch_size:] #last batch
      y_batch=y_shuffle[num_batches*batch_size:] #last batch
      resid=X_batch@coeff-y_batch
      gradient=lr*((X_batch.T)@resid)/len(X_batch)+alpha*(2*(coeff>0)-1)+beta*coeff
      coeff=coeff-gradient 
  return coeff

def LinearPredict(X,coeff): #If X was scaled, then this will return scaled predictions
  return X@coeff

def MSE(pred,y):
  return np.sum((pred-y)**2)/len(y)

def sigmoid(t):
  return 1/(1+np.exp(-t))

def LogisticProbability(X,coeff):
  return sigmoid(X@coeff)

def LogisticPredict(X,coeff):
  return X@coeff>0

def accuracy(pred,y):
  return np.sum(pred==y)/len(y)

def LogLoss(probs,y):
  return np.sum(y*np.log(probs)+(1-y)*np.log(1-probs))/len(y)

def LogisticSGD(X,y,epochs,batch_size,lr,alpha=0,beta=0):
  '''Stochastic Gradient Descent for Logistic Regression'''
  #alpha=amount of L1 (Lasso) regularization
  #beta=amount of L2 (Ridge) regularization
  X=np.array(X) 
  y=np.array(y) 
  n=len(X)
  coeff=np.ones(X.shape[1]) 
  indices=np.arange(len(X))
  for i in range(epochs):
    np.random.seed(i)
    np.random.shuffle(indices)
    X_shuffle=X[indices] 
    y_shuffle=y[indices] 
    num_batches=n//batch_size
    for j in range(num_batches):
      X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]
      y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]
      probs=LogisticProbability(X_batch,coeff)
      grad=X_batch.T@(probs-y_batch)/len(X_batch)
      gradient=lr*grad+alpha*(2*(coeff>0)-1)+beta*coeff
      coeff=coeff-gradient 
    if n%batch_size!=0: #Check if there is a smaller leftover batch
      X_batch=X_shuffle[num_batches*batch_size:] #last batch
      y_batch=y_shuffle[num_batches*batch_size:] #last batch
      probs=LogisticProbability(X_batch,coeff)
      grad=X_batch.T@(probs-y_batch)/len(X_batch)
      gradient=lr*grad+alpha*(2*(coeff>0)-1)+beta*coeff
      coeff=coeff-gradient
  return coeff

df = pd.read_csv('Masterdataset.csv')
df[['time_based_sampling_points','Q_daily_hunger_coded']]
df = df.dropna()
df[['time_based_sampling_points','Q_daily_hunger_coded']]
df

df['hrs_since_last_meal'] = 0

df.columns.get_loc('responseTime')

from IPython.lib.display import html_escape
#we presume that a person will eat breakfast at 8, lunch at 12, and dinner at 6

# time since last meal

def add_time_difference():

  for time in range(0, len(df)):

    hms = df.iloc[time, df.columns.get_loc('response_Time')]

    date = datetime.datetime.strptime(hms, "%H:%M:%S")

    date_in_hours = date.hour + date.minute/60 + date.second/3600

    if date_in_hours < 8:
      #if haven't had dinner before
      if df.iloc[time, df.columns.get_loc('eating_habits_1_dinner')] == 0:
        df.iloc[time, df.columns.get_loc('hrs_since_last_meal')] = 12 + date_in_hours

      #if had the dinner
      else:
        df.iloc[time, df.columns.get_loc('hrs_since_last_meal')] = 6 + date_in_hours

      
    elif date_in_hours > 8 and date_in_hours < 12:
      #if haven't had breakfast
      if df.iloc[time, df.columns.get_loc('eating_habits_1_breakfast')] == 0:
        df.iloc[time, df.columns.get_loc('hrs_since_last_meal')] = date_in_hours + 6
      #if had breakfast
      else:
        df.iloc[time, df.columns.get_loc('hrs_since_last_meal')] = date_in_hours - 8
    elif date_in_hours > 18:
      #if haven't had lunch
      if df.iloc[time, df.columns.get_loc('eating_habits_1_lunch')] == 0:
        df.iloc[time, df.columns.get_loc('hrs_since_last_meal')] = date_in_hours - 8
      #if had a lunch
      else:
        df.iloc[time, df.columns.get_loc('hrs_since_last_meal')] = date_in_hours - 12
      
add_time_difference()
df

df['Negative_Affect_MEAN']

X=np.array(df[['hrs_since_last_meal','Q_daily_hunger_coded']])

df['Negative_Affect_MEAN'] = df['Negative_Affect_MEAN'].replace(' ','15')

df['Negative_Affect_MEAN'] = pd.to_numeric(df['Negative_Affect_MEAN'])

df['Negative_Affect_MEAN'] = np.where(df['Negative_Affect_MEAN'] > 20, True, False)

Y = df['Negative_Affect_MEAN']

Z=X.astype(str)
Z = np.char.replace(Z,' ','2', count=None)

def sigmoid(t):
  return 1/(1+np.exp(-t))

def proba(Z,coeff): 
  Z = np.array(Z).astype(float)
  z = Z @ coeff
  return sigmoid(z)

m, n, b = -2, 0.5, 10
Z_with_ones = AddOnes(Z)
coeff = [m, n, b]
probs = proba(Z_with_ones, coeff)
probs

Z_with_ones = Z_with_ones.astype(np.float)

Z_with_ones

newcoeff=LogisticSGD(Z_with_ones,Y,2000,50,0.01)

time_c, hungry_c, inter = newcoeff

newcoeff

time, hungry = 8, 3

negative_effect = time_c * time + hungry_c * hungry + inter

negative_effect

def func(x):
  m=-newcoeff[0]/newcoeff[1]
  b=-newcoeff[2]/newcoeff[1]
  return m*x+b

cdict={'versicolor':'r','setosa':'g','virginica':'b'}
colors=df.apply(lambda x:cdict[x.["Negative_Affect_MEAN"]],axis=1)

plt.scatter(X[:,0],X[:,1],c=colors)
plt.plot(X[:,0],func(X[:,0]))